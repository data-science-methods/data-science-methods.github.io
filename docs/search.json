[
  {
    "objectID": "requirements.html",
    "href": "requirements.html",
    "title": "Requirements",
    "section": "",
    "text": "Labs\n\nThere are 6 labs. For the labs, you need to:\n\nSubmit all 6 labs\nRevise and resubmit at least 4 labs, as necessary, until they pass all automatic checks\n\n\nProject\n\nThe course project is divided into stages, with soft deadlines throughout the semester. Detailed instructions and guidelines are here."
  },
  {
    "objectID": "requirements.html#soft-deadlines",
    "href": "requirements.html#soft-deadlines",
    "title": "Requirements",
    "section": "Soft deadlines",
    "text": "Soft deadlines\nUntil the end of term, all deadlines are soft, with no penalties for turning in assignments a few days late. Labs have a soft deadline of 2 weeks for an initial submission, with an additional week to revise & resubmit as necessary to satisfy the automatic checks.\n\nI’ve found soft deadlines work well for about 80% students, who need the flexibility to handle ::gestures at the entire state of everything::. If soft deadlines don’t work well for you, treat the soft deadlines as hard deadlines. I will also try to follow up with you if I notice you’re missing a lot of earlier work.\n\n\nThis course uses a version of contract grading. This means:\n\nYour grade will be determined by the work you complete, not an assessment of the quality of your work.\nThere are only two possible grades: completed (A) and incomplete (B).\n\nIn exceptional cases, eg, almost no work completed, I might also assign a failing grade.\n\n\n\nContract grading was originally developed in writing courses, where the primary goal was to align grading with explicit (“objective”) measures of effort or productivity rather than tacit (“subjective”) measures of quality. Simply practicing writing a lot is more valuable for most undergraduate students than trying to write well. Contract grading also simplifies the grading process.\nI hate the term “contract grading,” which reinforces the idea that education is a commodity that you, the student, are purchasing from me, the teacher. The student-written “contract” of contract grading also seems basically unnecessary."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods of Data Science I",
    "section": "",
    "text": "GRAN 120\nTÞ 12:00-1:15pm\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n  \n  \n  \n    \n      week\n      date\n      topic\n      readings\n      assignment\n    \n  \n  \n    \n      Some fundamental tools\n    \n    1\nAug 25\nMotivation: What researchers can learn from software engineering\n\n\nMcElreath, Richard. 2020. Science as Amateur Software Development. https://www.youtube.com/watch?v=zwRdO9_GGhY.\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510.\n\n\n\n    2\nAug 30\nFundamental tools: git, tests, and R Markdown\n\n\nRam, Karthik. 2013. “Git Can Facilitate Greater Reproducibility and Increased Transparency in Science.” Source Code for Biology and Medicine 8 (1): 7. https://doi.org/10.1186/1751-0473-8-7.\nOptional: Blischak, John D., Emily R. Davenport, and Greg Wilson. 2016. “A Quick Introduction to Version Control with Git and GitHub.” PLOS Computational Biology 12 (1): e1004668. https://doi.org/10.1371/journal.pcbi.1004668.\nOptional: Baumer, Ben, Mine Cetinkaya-Rundel, Andrew Bray, Linda Loi, and Nicholas J. Horton. 2014. “R Markdown: Integrating A Reproducible Analysis Tool into Introductory Statistics.” ArXiv:1402.1894 [Stat], February. http://arxiv.org/abs/1402.1894.\n\n\nLab Week 2: Git\n\n    3\nSep 06\nFundamental techniques: Debugging errors and getting help\n\n\n“R Faq - How to Make a Great R Reproducible Example.” n.d. Stack Overflow. Accessed August 31, 2018. https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example.\nBryan, Jenny. 2020. “Object of Type ‘Closure’ Is Not Subsettable.” Presented at the RSTUDIO::CONF 2020, January 31. https://rstudio.com/resources/rstudioconf-2020/object-of-type-closure-is-not-subsettable/.\nWickham, Hadley. 2019. “Debugging.” In Advanced R, Second Edition, chap. 22. CRC Press. https://adv-r.hadley.nz/debugging.html.\n\n\nLab Week 3: Debugging\n\n    4\nSep 13\nFundamental techniques: Programming paradigms\n\n\nIntroduction to the \"Functional Programming\" section of Advanced R: https://adv-r.hadley.nz/fp.html\nIntroduction to the \"Object-oriented Programming\" section of Advanced R: https://adv-r.hadley.nz/oo.html\nChambers, John M. 2014. “Object-Oriented Programming, Functional Programming and R.” Statistical Science 29 (2): 167–80. https://doi.org/10.1214/13-STS452. (Non-paywalled copy)\n\n\nLab Week 4: Programming paradigms\n\n    5\nSep 20\nData journeys (Covid contrarianism I)\n\n\nLeonelli, Sabina. 2020. “Learning from Data Journeys.” In Data Journeys in the Sciences, edited by Sabina Leonelli and Niccolò Tempini. Cham, Switzerland: Springer Nature. https://link.springer.com/chapter/10.1007/978-3-030-37177-7_1.\nMadrigal, Robinson Meyer, Alexis C. 2021. “Why the Pandemic Experts Failed.” The Atlantic. March 15, 2021. https://www.theatlantic.com/science/archive/2021/03/americas-coronavirus-catastrophe-began-with-data/618287/.\nBanco, Erin. 2021. “Inside America’s Covid-Reporting Breakdown.” POLITICO. August 15, 2021. https://www.politico.com/news/2021/08/15/inside-americas-covid-data-gap-502565.\n\n\nProject: Proposal\n\n    \n      Exploratory data analysis\n    \n    6\nSep 27\nEDA\n\n\nch. 7, \"Phenomena,\" of Brown, James Robert. 2002. Smoke and Mirrors: How Science Reflects Reality. Routledge. link\nchs. 2 and 4 of Peng, Roger D., and Elizabeth Matsui. 2016. The Art of Data Science: A Guide for Anyone Who Works with Data. Leanpub. link\nHuebner, Vach, and le Cessie. 2016. \"A Systematic Approach to Initial Data Analysis Is Good Research Practice.\" The Journal of Thoracic and Cardiovascular Surgery 151 (1): 25-27. https://doi.org/10.1016/j.jtcvs.2015.09.085.\n\n\nLab Week 6: EDA\n\n    7\nOct 04\nEDA\n\nNo new readings\n\n\n    8\nOct 11\nCovid contrarianism II\n\n\nLee, Crystal, Tanya Yang, Gabrielle Inchoco, Graham M. Jones, and Arvind Satyanarayan. 2021. “Viral Visualizations: How Coronavirus Skeptics Use Orthodox Data Practices to Promote Unorthodox Science Online.” Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, May, 1–18. https://doi.org/10.1145/3411764.3445211.\nDouglass, Rex W. 2020. “How to Be Curious Instead of Contrarian About COVID-19: Eight Data Science Lessons From ‘Coronavirus Perspective’ (Epstein 2020).” March 30, 2020. Link.\n\n\nProject: Data Journey Narrative\n\n    \n      Reproducibility and replicability\n    \n    9\nOct 18\nCode review\n\n\nBryan, Jenny. 2018. “Code Smells and Feels.” Presented at the useR 2018, July 21. https://www.youtube.com/watch?v=7oyiPBjLAWY.\nPostolovski, Tash. 2020. “Your Code Review Checklist: 14 Things to Include.” July 6, 2020. Link.\nBuckens, Wouter. 2019. “Self-Documenting Is a Myth, and How to Make Your Code Self-Documenting.” Woubuc. August 3, 2019. Link.\nNote: We'll be doing code review and trying to reproduce Table 1 from this article. Read as background, not necessarily for a full understanding. Zhou, Xiaodan, Kevin Josey, Leila Kamareddine, Miah C. Caine, Tianjia Liu, Loretta J. Mickley, Matthew Cooper, and Francesca Dominici. 2021. “Excess of COVID-19 Cases and Deaths Due to Fine Particulate Matter Exposure during the 2020 Wildfires in the United States.” Science Advances 7 (33): eabi8789. https://doi.org/10.1126/sciadv.abi8789.\n\n\nLab Week 10: Code Review\n\n    10\nOct 25\nProject management and sharing data\n\n\nNote: On Tuesday, we'll discuss your code review of Zhou et al. (2021)\nLaskowski, Kate. n.d. “What to Do When You Don’t Trust Your Data Anymore – Laskowski Lab at UC Davis.” Accessed January 29, 2020. Link.\nBailey, David H., and Jonathan Borwein (Jon). n.d. “The Reinhart-Rogoff Error – or How Not to Excel at Economics.” The Conversation. Accessed May 15, 2020. Link.\ncaitlinhudon. 2018. “Field Notes: Building Data Dictionaries.” Haystacks (blog). October 30, 2018. https://caitlinhudon.com/2018/10/30/data-dictionaries/.\nNoble, William Stafford. 2009. “A Quick Guide to Organizing Computational Biology Projects.” PLOS Computational Biology 5 (7): e1000424. https://doi.org/10.1371/journal.pcbi.1000424.\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 1–9. https://doi.org/10.1038/sdata.2016.18.\n\n\nProject: EDA\n\n    11\nNov 01\nThe replication crisis\n\n\nHicks, Dan. 2021. Open Science Can’t Solve the Replication Crisis. DS2 2021. CEFISES at UCLouvain. https://www.youtube.com/watch?v=uheU_W7_oz8.\nPatil, Prasad, Roger D. Peng, and Jeffrey T. Leek. 2019. “A Visual Tool for Defining Reproducibility and Replicability.” Nature Human Behaviour 3 (7): 650. https://doi.org/10.1038/s41562-019-0629-z.\nHardwicke, Tom E., Maya B. Mathur, Kyle MacDonald, Gustav Nilsonne, George C. Banks, Mallory C. Kidwell, Alicia Hofelich Mohr, et al. 2018. “Data Availability, Reusability, and Analytic Reproducibility: Evaluating the Impact of a Mandatory Open Data Policy at the Journal Cognition.” Royal Society Open Science 5 (8): 180448. https://doi.org/10.1098/rsos.180448.\nOptional: Jager, Leah R., and Jeffrey T. Leek. 2014. “An Estimate of the Science-Wise False Discovery Rate and Application to the Top Medical Literature.” Biostatistics 15 (1): 1–12. https://doi.org/10.1093/biostatistics/kxt007.\nOptional: Smaldino, Paul, Matthew Adam Turner, and Pablo Andrés Contreras Kallens. 2019. “Open Science and Modified Funding Lotteries Can Impede the Natural Selection of Bad Science.” Royal Society Open Science 6 (7): 190194. https://doi.org/10.1098/rsos.190194.\nOptional: Hicks, Daniel J. 2021. “Open Science, the Replication Crisis, and Environmental Public Health.” Accountability in Research. https://doi.org/10.1080/08989621.2021.1962713.\n\n\nLab Week 12: Reproducibility\n\n    12\nNov 08\nNo class: Election Day & Instructor at conference\n\n\n\n    13\nNov 15\nReproducibility tools: make\n\n\nHeil, Benjamin J., Michael M. Hoffman, Florian Markowetz, Su-In Lee, Casey S. Greene, and Stephanie C. Hicks. 2021. “Reproducibility Standards for Machine Learning in the Life Sciences.” Nature Methods, August, 1–4. https://doi.org/10.1038/s41592-021-01256-7.\nJones, Zach. 2013. “GNU Make for Reproducible Data Analysis.” November 2013. http://zmjones.com/make/.\n\n\n\n    14\nNov 22\nThanksgiving break\n\n\n\n    15\nNov 29\nmake\n\nNo new readings\n\n\n    16\nDec 06\nCovid contrarianism III\n\n\nLeonelli, Sabina. 2021. “Data Science in Times of Pan(Dem)Ic.” Harvard Data Science Review, January. https://doi.org/10.1162/99608f92.fbb1bdd6.\nJordan, Catherine, Susan Gust, and Naomi Scheman. 2011. “The Trustworthiness of Research: The Paradigm of Community-Based Research.” In Shifting Ground: Knowledge and Reality, Transgression and Trustworthiness, by Naomi Scheman, 170–90. Oxford University Press. Link\n\n\nProject: Report\n\n    \n      \n    \n    \nDec 13\nProject presentations\nHard deadline for everything\n\n\nProject: Presentation"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "reflexivity.html",
    "href": "reflexivity.html",
    "title": "Reflexivity questions",
    "section": "",
    "text": "What do I already know about this?\nWhy am I learning more about this?\nWhat do I expect or hope to learn or find?\nWho and what is affected by this topic? How can I respect them through the work I’m about to do?"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Course project",
    "section": "",
    "text": "Proposal\nWhat to submit: The proposal document, in the text of an email or as an attached plaintext, HTML, or PDF file\nYour proposal should formulate a research question for your project and identify a dataset that you’ll analyze to address your research question.\nYour research question can be a more formal, academic question — like something from your home discipline — or a more causal, “data science-y” question — like you might examine in a hackathon, data science challenge, or detailed blog post.\nThe dataset should be relatively large and complex, and not data that you have collected yourself or previously analyzed. Part of the work of researching the data journey and exploring data analysis is familiarizing yourself with found or opportunistic data. It’s okay if you’re using the dataset for a project in another class or your lab group this semester, so long as you’re asking a very different research question.\nBecause you’re working with opportunistic data, it might be easier to find an interesting dataset first, then develop your research question.\nIf you’re having trouble finding interesting datasets, try the TidyTuesday project or the Kaggle dataset library.\nThe proposal document should be 3 paragraphs long, with 75-150 words per paragraph. The paragraphs should be labeled “Intellectual Merit,” “Broader Impacts,” and “Data and Methods,” in that order.\n\nIntellectual Merit\n\nWhat is your primary research question? Why is this question interesting and worth investigating, from an academic or intellectual perspective?\n\nBroader Impacts\n\nWhy is your research question socially valuable? Why should the general public care about the answer to this question, and pay someone to investigate it?\n\nData and Methods\n\nWhat dataset will you use to investigate your question? Why does this dataset seem appropriate for your question? What analytical methods will you apply? Because it’s not the focus of our class, you don’t have to use any fancy statistical methods (for whatever “fancy” means to you). You can even focus on exploratory methods, such as descriptive summaries and visualization.\n\n\n\n\nData journey narrative\n[tbd]\n\n\nExploratory data analysis\nWhat to submit: A link to your GitHub repo (with access if it’s private). The EDA itself should have two analysis files: an R or Rmd script, and an html file generated from the script using knitr/Rmarkdown/Quarto. If your code requires anything more than simply running the script, include instructions in a README.\nYour exploratory analysis should cover the most relevant elements from the EDA checklists to validate your data. Be sure to identify and address limitations of your data for answering your research question.\nIf your EDA concludes that these data are not fit for purpose, it’s okay to continue forward with your project. Your report below should focus on explaining why your data are not fit for purpose, and then discuss how better data might be collected.\n\n\nCode review and reproducibility check\nWhat to submit: You’ll submit your code review by adding comments to the author’s code and then submitting a pull request. See the instructions below.\n\n\n\n\n\n\nCode review step-by-step instructions\n\n\n\n\n\n\nFork and clone the repository you’ll be reviewing\nAs you review the code, add comments directly to the code\n\nStart your comments in a distinctive way, eg, #DH#. Then the author can find your comments with a quick text search.\n\nWhen you’re finished, commit your changes (the comments), push, and file a PR against the author’s main branch\nAuthor and reviewer can use GitHub’s discussion features on the PR to ask followup questions or clarify comments\nAs you address the comments from your reviewer, remove their comments and note “done” in the PR. Then a quick text search and review of the PR can confirm once you’ve addressed all the comments.\n\n\n\n\nAs in the code review lab, your code review should cover the most relevant checklist elements. Specifically, be sure to address:\n\nIs it clear what steps you need to take to run the script?\nWhen you follow those steps, does the script run?\nIs the html file from the repository reproducible?\n\nHow easy or difficult is it to read the script and understand what the author is doing and why?\n\n\n\n(Reproducible) report\nWhat to submit: A PDF of your report, a link or invitation to your GitHub repo with automatically reproducible code and data\nThe report for your analysis should structured somewhat like a traditional scientific paper: introduction, methods, results, discussion. But the content of these sections will be somewhat non-traditional.\nThe total length of this report should be 3,000-5,000 words, not including references.\nAs a stretch goal, your report should be fully reproducible. Using renv and a Makefile, I should be able to clone your repository, run renv::restore() and then make, and exactly reproduce the PDF of your report.\n\nIntroduction\n\nAs usual, motivate your topic, give some background, and clearly state the research question.\nIn addition, incorporate answers to the reflexivity questions: what did you already know about this topic before starting to work on the project, what did you expect to find, who’s impacted by this topic, and how will your work respect them?\n\nMethods\n\nThe methods section should focus on the data, and in particular should incorporate your data journey narrative.\n\nResults\n\nAs usual, your findings should be framed as phenomena (in the sense of the Brown chapter) rather than causal claims, mechanisms, or theoretical results.\nThe first subsection of your findings should incorporate your EDA. The second subsection can address your research questions. Use visualizations along with or instead of tables. Keep your visualizations close to the data. For example, include both a scatterplot and a fitted regression curve.\n\nDiscussion\n\nAs usual, briefly (1-2 sentences) recap your major findings, then discuss limitations and directions for future research.\nResist the temptation to treat your findings as conclusive or as vindicating (or challenging) some larger theory. Instead, emphasize the ways future research can improve our understanding of the phenomena: how new data might be collected to mitigate the limitations of this dataset, how different kinds of data might complement the kind of data used here, and how further studies might trace out the scope of the phenomena found in this dataset.\nFinally, come back to the reflexivity questions in the introduction. Were your expectations met? (Probably a mix of both yes and no; explain.) What implications might your findings have for people affected by the topic? How should these implications shape the trajectory of future research on this topic?\n\n\n\n\nFlash talk presentation\nWhat to submit: You won’t submit anything, but you will give a presentation for the class\nAt the last class meeting, you’ll give a 5-minute presentation of your project. Your target audience is the other students in the class, so you’ll need to start by motivating your project and be thoughtful about the use of jargon from your home discipline. Depending on how much time we have, we may or may not have questions after the flash talks.\nYou should use slides for your presentation, but you don’t have to turn them in. Google Slides, PowerPoint, Keynote, or whatever are fine, because you’re responsible for arranging to project your slides and checking in advance that everything works as expected in our classroom.\nBecause flash talks are so short, I strongly recommend writing a script, checking the timing, and then memorizing the script. There will be a loud timer at 5 minutes."
  }
]