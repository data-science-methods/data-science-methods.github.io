[
  {
    "objectID": "requirements.html",
    "href": "requirements.html",
    "title": "Requirements",
    "section": "",
    "text": "Prerequisites\nThis course assumes basic competence with introductory R.\n\n“Introductory R”\n\nLessons 1-5 of the Carpentries “R for Social Scientists” curriculum\n\nInstalling R and packages\nWorking in the R Studio IDE\nCommon data types\nReading and writing CSV files\nTidyverse R: mutate(), filter(), select(); plotting with ggplot2\n\n\n“Basic competence”\n\nGiven time and a reference (cheatsheet, Stack Exchange, mentor) you can figure out how to solve a problem\n\n\n\n\nAssignments\n\nLabs\n\nThere are 6 labs. For the labs, you need to:\n\nSubmit all 6 labs\nRevise and resubmit at least 4 labs, as necessary, until I accept them (usually based on automatic checks)\n\n\n\n\n\n\n\nlab\ndeadline\n\n\n\n\n1\nGit\nSep 15\n\n\n2\nDebugging\nSep 22\n\n\n3\nFunctional Programming\nSep 29\n\n\n4\nEDA\nOct 20\n\n\n5\nCode Review\nDec 1\n\n\n6\nReproducibility\nDec 1\n\n\n\n\nProject\n\nThe course project is divided into stages. Detailed instructions and guidelines are here.\n\n\n\n\n\n\nstage\ndeadline\n\n\n\n\n1\nProposal\nSept 22\n\n\n2\nData journey narrative\nOct 13\n\n\n3\nExploratory data analysis\nNov 17\n\n\n4\nCode review\nDec 1\n\n\n5\nReproducible report\nDec 13\n\n\n6\nFlash talk presentation\nDec 13\n\n\n\n\nThe usual\n\nDo the assigned reading, come to class prepared to discuss it, contribute to class and your lab collaborations, and so on.\n\n\n\n\nPedagogical note\nThis course uses a version of contract grading. This means:\n\nYour grade will be determined by the work you complete, not an assessment of the quality of your work.\nThere are only two possible grades: completed (A) and incomplete (B).\n\nIn exceptional cases, eg, almost no work completed, I might also assign a failing grade.\n\n\nContract grading was originally developed in writing courses, where the primary goal was to align grading with explicit (“objective”) measures of effort or productivity rather than tacit (“subjective”) measures of quality. Simply practicing writing a lot is more valuable for most undergraduate students than trying to write well. Contract grading also simplifies the grading process.\nI hate the term “contract grading,” which reinforces the idea that education is a commodity that you, the student, are purchasing from me, the teacher. The student-written “contract” of contract grading also seems basically unnecessary."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods of Data Science I",
    "section": "",
    "text": "Fall 2023\nUC Merced\nProf. Dan Hicks (they/them)\ndhicks4@ucmerced.edu or hicks.daniel.j@gmail.com\n\nGRAN 140\nTÞ 1:30-2:45pm\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n  \n  \n  \n    \n      week\n      date\n      topic\n      readings\n      assignment\n    \n  \n  \n    \n      Some fundamental tools\n    \n    1\nAug 24\nMotivation: What researchers can learn from software engineering\n\n\nMcElreath, Richard. 2020. Science as Amateur Software Development. https://www.youtube.com/watch?v=zwRdO9_GGhY.\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510.\n\n\n\n    2\nAug 29\ngit and R Markdown\n\n\nRam, Karthik. 2013. “Git Can Facilitate Greater Reproducibility and Increased Transparency in Science.” Source Code for Biology and Medicine 8 (1): 7. https://doi.org/10.1186/1751-0473-8-7.\nOptional: Blischak, John D., Emily R. Davenport, and Greg Wilson. 2016. “A Quick Introduction to Version Control with Git and GitHub.” PLOS Computational Biology 12 (1): e1004668. https://doi.org/10.1371/journal.pcbi.1004668.\nOptional: Baumer, Ben, Mine Cetinkaya-Rundel, Andrew Bray, Linda Loi, and Nicholas J. Horton. 2014. “R Markdown: Integrating A Reproducible Analysis Tool into Introductory Statistics.” ArXiv:1402.1894 [Stat], February. http://arxiv.org/abs/1402.1894.\n\n\nLab 1: Git\n\n    3\nSep 05\nFunctions and debugging\n\n\n“R Faq - How to Make a Great R Reproducible Example.” n.d. Stack Overflow. Accessed August 31, 2018. https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example.\nBryan, Jenny. 2020. “Object of Type ‘Closure’ Is Not Subsettable.” Presented at the RSTUDIO::CONF 2020, January 31. https://rstudio.com/resources/rstudioconf-2020/object-of-type-closure-is-not-subsettable/.\nWickham, Hadley. 2019. “Debugging.” In Advanced R, Second Edition, chap. 22. CRC Press. https://adv-r.hadley.nz/debugging.html.\n\n\nLab 2: Debugging\n\n    4\nSep 12\nData journeys\n\n\nBates, Jo, Yu-Wei Lin, and Paula Goodale. 2016. “Data Journeys: Capturing the Socio-Material Constitution of Data Objects and Flows.” Big Data & Society 3 (2): 2053951716654502. https://doi.org/10.1177/2053951716654502.\nMadrigal, Robinson Meyer, Alexis C. 2021. “Why the Pandemic Experts Failed.” The Atlantic. March 15, 2021. https://www.theatlantic.com/science/archive/2021/03/americas-coronavirus-catastrophe-began-with-data/618287/.\nOptional: Banco, Erin. 2021. “Inside America’s Covid-Reporting Breakdown.” POLITICO. August 15, 2021. https://www.politico.com/news/2021/08/15/inside-americas-covid-data-gap-502565.\nOptional: Reveal. 2023. \"The COVID Tracking Project.\" https://revealnews.org/article/covid-tracking-project/ (three-episode podcast series)\n\n\nLab 3: Functional programming\n\n    5\nSep 19\nWork week\n\n\nNo new reading\nIn-class time to work on labs and projects\n\n\nProject: Proposal\n\n    \n      Exploratory data analysis\n    \n    6\nSep 26\nEDA\n\n\nch. 7, \"Phenomena,\" of Brown, James Robert. 2002. Smoke and Mirrors: How Science Reflects Reality. Routledge. link\nchs. 2 and 4 of Peng, Roger D., and Elizabeth Matsui. 2016. The Art of Data Science: A Guide for Anyone Who Works with Data. Leanpub. link\nOptional: Huebner, Vach, and le Cessie. 2016. \"A Systematic Approach to Initial Data Analysis Is Good Research Practice.\" The Journal of Thoracic and Cardiovascular Surgery 151 (1): 25-27. https://doi.org/10.1016/j.jtcvs.2015.09.085.\nOptional: Zuur, Alain F., Elena N. Ieno, and Chris S. Elphick. 2010. “A Protocol for Data Exploration to Avoid Common Statistical Problems.” Methods in Ecology and Evolution 1 (1): 3–14. https://doi.org/10.1111/j.2041-210X.2009.00001.x.\n\n\n\n    7\nOct 03\nEDA\n\nNo new readings\n\nLab 4: EDA\n\n    8\nOct 10\nCovid contrarianism\n\n\nLee, Crystal, Tanya Yang, Gabrielle Inchoco, Graham M. Jones, and Arvind Satyanarayan. 2021. “Viral Visualizations: How Coronavirus Skeptics Use Orthodox Data Practices to Promote Unorthodox Science Online.” Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, May, 1–18. https://doi.org/10.1145/3411764.3445211.\nDouglass, Rex W. 2020. “How to Be Curious Instead of Contrarian About COVID-19: Eight Data Science Lessons From ‘Coronavirus Perspective’ (Epstein 2020).” March 30, 2020. Link.\n\n\nProject: Data Journey Narrative\n\n    9\nOct 17\nWork week\n\n\n\n    \n      Reproducibility and replicability\n    \n    10\nOct 24\nThe replication crisis\n\n\nHicks, Dan. 2021. \"Open science, the replication crisis, and environmental public health.\" Accountability in Research. https://doi.org/10.1080/08989621.2021.1962713.\nBailey, David H., and Jonathan Borwein (Jon). n.d. “The Reinhart-Rogoff Error – or How Not to Excel at Economics.” The Conversation. Accessed May 15, 2020. Link.\nOptional: Jager, Leah R., and Jeffrey T. Leek. 2014. “An Estimate of the Science-Wise False Discovery Rate and Application to the Top Medical Literature.” Biostatistics 15 (1): 1–12. https://doi.org/10.1093/biostatistics/kxt007.\nOptional: Smaldino, Paul, Matthew Adam Turner, and Pablo Andrés Contreras Kallens. 2019. “Open Science and Modified Funding Lotteries Can Impede the Natural Selection of Bad Science.” Royal Society Open Science 6 (7): 190194. https://doi.org/10.1098/rsos.190194.\n\n\n\n    11\nOct 31\nCode review\n\n\nBryan, Jenny. 2018. “Code Smells and Feels.” Presented at the useR 2018, July 21. https://www.youtube.com/watch?v=7oyiPBjLAWY.\nPostolovski, Tash. 2020. “Your Code Review Checklist: 14 Things to Include.” July 6, 2020. Link.\nBuckens, Wouter. 2019. “Self-Documenting Is a Myth, and How to Make Your Code Self-Documenting.” Woubuc. August 3, 2019. Link.\nNote: We'll be doing code review and trying to reproduce Table 1 from the article below. Read as background, not necessarily for a full understanding.\nZhou, Xiaodan, Kevin Josey, Leila Kamareddine, Miah C. Caine, Tianjia Liu, Loretta J. Mickley, Matthew Cooper, and Francesca Dominici. 2021. “Excess of COVID-19 Cases and Deaths Due to Fine Particulate Matter Exposure during the 2020 Wildfires in the United States.” Science Advances 7 (33): eabi8789. https://doi.org/10.1126/sciadv.abi8789.\n\n\nLab 5: Code Review\n\n    12\nNov 07\nProject management and sharing data\n\n\nLaskowski, Kate. n.d. “What to Do When You Don’t Trust Your Data Anymore – Laskowski Lab at UC Davis.” Accessed January 29, 2020. Link.\ncaitlinhudon. 2018. “Field Notes: Building Data Dictionaries.” Haystacks (blog). October 30, 2018. https://caitlinhudon.com/2018/10/30/data-dictionaries/.\nNoble, William Stafford. 2009. “A Quick Guide to Organizing Computational Biology Projects.” PLOS Computational Biology 5 (7): e1000424. https://doi.org/10.1371/journal.pcbi.1000424.\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 1–9. https://doi.org/10.1038/sdata.2016.18.\n\n\nLab 6: Reproducibility\n\n    13\nNov 14\nWork week\n\n\nProject: EDA\n\n    14\nNov 21\nThanksgiving break\n\n\n\n    15\nNov 28\nFailure\n\n\nOn Tuesday we'll be discussing Lab 6\nRedish, A. David, Erich Kummerfeld, Rebecca Lea Morris, and Alan C. Love. 2018. “Opinion: Reproducibility Failures Are Essential to Scientific Inquiry.” Proceedings of the National Academy of Sciences 115 (20): 5042–46. https://doi.org/10.1073/pnas.1806370115.\nFeest, Uljana. 2019. “Why Replication Is Overrated.” Philosophy of Science 86 (5): 895–905. https://doi.org/10.1086/705451.\n\n\nProject: Code Review\n\n    16\nDec 05\nData justice\n\n\nJohnson, Kaneesha R. 2021. \"Two Regimes of Prison Data Collection.\" Harvard Data Science Review, Summer. https://doi.org/10.1162/99608f92.72825001\nOptional: Rodriguez-Lonebear, Desi. 2016. “Building a Data Revolution in Indian Country.” In Indigenous Data Sovereignty, edited by Tahu Kukutai and John Taylor, 1st ed. ANU Press. https://doi.org/10.22459/CAEPR38.11.2016.14.\n\n\n\n    \n      \n    \n    \nDec 13\nProject presentations (9:30am, GRAN 140)\nHard deadline for everything\n\n\nProject: Report & Presentation"
  },
  {
    "objectID": "reflexivity.html",
    "href": "reflexivity.html",
    "title": "Reflexivity questions",
    "section": "",
    "text": "What do I already know about this?\nWhy am I learning more about this?\nWhat do I expect or hope to learn or find?\nWho and what is affected by this topic? How can I respect them through the work I’m about to do?"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Course project",
    "section": "",
    "text": "In this project, you’ll practice and synthesize your data science skills. The project is scaffolded into 6 stages, with due dates throughout the term. The project is progressive and iterative: you’ll get feedback from me after each stage, and be able to rework previous stages if necessary as you move forward.\n\nProposal\nWhat to submit: The proposal document, in the text of an email or as an attached plaintext, HTML, or PDF file\nYour proposal should formulate a research question for your project and identify a dataset that you’ll analyze to address your research question.\nYour research question can be a more formal, academic question — like something from your home discipline — or a more causal, “data science-y” question — like you might examine in a hackathon, data science challenge, or detailed blog post.\nThe dataset should be relatively large and complex, and not data that you have collected yourself or previously analyzed. Part of the work of researching the data journey and exploring data analysis is familiarizing yourself with found or opportunistic data. It’s okay if you’re using the dataset for a project in another class or your lab group this semester, so long as you’re asking a very different research question.\nBecause you’re working with opportunistic data, it might be easier to find an interesting dataset first, then develop your research question.\nIf you’re having trouble finding interesting datasets, try the TidyTuesday project or the Kaggle dataset library.\nThe proposal document should be 3 paragraphs long, with 75-150 words per paragraph. The paragraphs should be labeled “Intellectual Merit,” “Broader Impacts,” and “Data and Methods,” in that order.\n\nIntellectual Merit\n\nWhat is your primary research question? Why is this question interesting and worth investigating, from an academic or intellectual perspective?\n\nBroader Impacts\n\nWhy is your research question socially valuable? Why should the general public care about the answer to this question, and pay someone to investigate it?\n\nData and Methods\n\nWhat dataset will you use to investigate your question? Why does this dataset seem appropriate for your question? What analytical methods will you apply? Because it’s not the focus of our class, you don’t have to use any fancy statistical methods (for whatever “fancy” means to you). You can even focus on exploratory methods, such as descriptive summaries and visualization.\n\n\n\n\nData journey narrative\nWhat to submit: A document, by email, preferably as an attached plaintext, HTML, or PDF file.\nCharacterize the journey that your data took to get to you. Some relevant questions to answer might include\n\nWho generated these data? Why? How?\nWhat measurement instruments were used to generate these data? What infrastructure has been used to store and transmit the data?\nWhat was the original intended use of these data? What are some other ways the data have been used?\nHow has the mutability/immutability of the data been important to its journey & use? How has its mutability/immutability been created and maintained?\n\nHow will you need to transform the data further for your project? How do the physical and material properties of the data facilitate this, and/or create friction?\n\nWhat values have been crystallized in the data? How have power relations and values shaped what is/isn’t included in the data?\nBased on these factors, do the data appear to be fit for purpose? That is, based on what you know so far, do the data appear to be appropriate for answering your research question?\n\nInclude references for the sources you use to answer these questions. You can work on this step of the project at the same time as the EDA step, but I will ask you to turn in the data journey narrative first. A good target length for your narrative is 500-1,000 words long, not counting any references.\n\n\nExploratory data analysis\nWhat to submit: A link to your GitHub repo (with access if it’s private). The EDA itself should have two analysis files: an R or Rmd script, and an html file generated from the script using knitr/Rmarkdown/Quarto. If your code requires anything more than simply running the script, include instructions in a README. Your code from this step will go through peer review in the next step.\nYour exploratory analysis should cover the most relevant elements from the EDA checklists to validate your data. Be sure to identify and address limitations of your data for answering your research question.\nUse a literate programming style, mixing code with expository text. Julia Silge has several good examples of blog tutorials written in this style:\n\nSliding windows for #TidyTuesday rents in San Francisco\nReordering and facetting for ggplot2\n\nIf your EDA concludes that these data are not fit for purpose, it’s okay to continue forward with your project. Your report below should focus on explaining why your data are not fit for purpose, and then discuss how better data might be collected.\n\n\nCode review and reproducibility check\nWhat to submit: You’ll submit your code review by adding comments to the author’s code and then submitting a pull request. See the instructions below.\n\n\n\n\n\n\nCode review step-by-step instructions\n\n\n\n\n\n\nFork and clone the repository you’ll be reviewing\nAs you review the code, add comments directly to the code\n\nStart your comments in a distinctive way, eg, #DH#. Then the author can find your comments with a quick text search.\n\nWhen you’re finished, commit your changes (the comments), push, and file a PR against the author’s main branch\nAuthor and reviewer can use GitHub’s discussion features on the PR to ask followup questions or clarify comments\nAs you address the comments from your reviewer, remove their comments and note “done” in the PR. Then a quick text search and review of the PR can confirm once you’ve addressed all the comments.\n\n\n\n\nAs in the code review lab, your code review should cover the most relevant checklist elements. Specifically, be sure to address:\n\nIs it clear what steps you need to take to run the script?\nWhen you follow those steps, does the script run?\nIs the html file from the repository reproducible?\n\nHow easy or difficult is it to read the script and understand what the author is doing and why?\n\n\n\n(Reproducible) report\nWhat to submit: A PDF of your report, a link or invitation to your GitHub repo with automatically reproducible code and data\nThe report for your analysis should structured somewhat like a traditional scientific paper: introduction, methods, results, discussion. But the content of these sections will be somewhat non-traditional.\nThe total length of this report should be 3,000-5,000 words, not including references.\nAs a stretch goal, your report should be fully reproducible. Using renv and a Makefile, I should be able to clone your repository, run renv::restore() and then make, and exactly reproduce the PDF of your report.\n\nIntroduction\n\nAs usual, motivate your topic, give some background, and clearly state the research question.\nIn addition, incorporate answers to the reflexivity questions: what did you already know about this topic before starting to work on the project, what did you expect to find, who’s impacted by this topic, and how will your work respect them?\n\nMethods\n\nThe methods section should focus on the data, and in particular should incorporate your data journey narrative.\n\nResults\n\nAs usual, your findings should be framed as phenomena (in the sense of the Brown chapter) rather than causal claims, mechanisms, or theoretical results.\nThe first subsection of your findings should incorporate your EDA. The second subsection can address your research questions. Use visualizations along with or instead of tables. Keep your visualizations close to the data. For example, include both a scatterplot and a fitted regression curve.\n\nDiscussion\n\nAs usual, briefly (1-2 sentences) recap your major findings, then discuss limitations and directions for future research.\nResist the temptation to treat your findings as conclusive or as vindicating (or challenging) some larger theory. Instead, emphasize the ways future research can improve our understanding of the phenomena: how new data might be collected to mitigate the limitations of this dataset, how different kinds of data might complement the kind of data used here, and how further studies might trace out the scope of the phenomena found in this dataset.\nFinally, come back to the reflexivity questions in the introduction. Were your expectations met? (Probably a mix of both yes and no; explain.) What implications might your findings have for people affected by the topic? How should these implications shape the trajectory of future research on this topic?\n\n\n\n\nFlash talk presentation\nWhat to submit: You won’t submit anything, but you will give a presentation for the class\nAt the last class meeting, you’ll give a 5-minute presentation of your project. Your target audience is the other students in the class, so you’ll need to start by motivating your project and be thoughtful about the use of jargon from your home discipline. Depending on how much time we have, we may or may not have questions after the flash talks.\nYou should use slides for your presentation, but you don’t have to turn them in. Google Slides, PowerPoint, Keynote, or whatever are fine, because you’re responsible for arranging to project your slides and checking in advance that everything works as expected in our classroom.\nBecause flash talks are so short, I strongly recommend writing a script, checking the timing, and then memorizing the script. There will be a loud timer at 5 minutes."
  }
]